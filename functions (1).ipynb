{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from urllib.parse import urlparse, urlsplit, urlunsplit\n",
    "from time import sleep\n",
    "from datetime import date\n",
    "from networkx import density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresExtraction(features_file):\n",
    "    convertedArray = json.load(open(features_file, 'r'))\n",
    "    features = []\n",
    "    for i, j in enumerate(convertedArray):\n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            features.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            features.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            features.append(j['text'])\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelExtraction(label_file):\n",
    "#     csv_file = open(file)\n",
    "#     csv_reader = csv.reader(csv_file, delimiter = ' ')\n",
    "#     labels = []\n",
    "#     for row in csv_reader:\n",
    "#         labels.append(row[0])\n",
    "    labels = np.genfromtxt(label_file,dtype=None)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction(array, key):\n",
    "    reducedArray = []\n",
    "    for i, j in enumerate(array):\n",
    "        if len(reducedArray) == 0:\n",
    "            reducedArray.append(j)\n",
    "        else:\n",
    "            count = 0\n",
    "            for k in range(len(reducedArray)):\n",
    "                if a[k] != b[i]:\n",
    "                    count = count + 1\n",
    "            \n",
    "                if count == len(a):\n",
    "                    a.append(b[i])\n",
    "    \n",
    "    for i, j in enumerate(convertedArray):\n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            features.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            features.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            features.append(j['text'])\n",
    "    return reducedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCredentials():\n",
    "    app_key = \"jGVy1ilV55A7P24eJHuousM1b\"\n",
    "    app_secret = \"wrPOxGirrXEUQsU7Pk0jqaqt7nLYq10TmR0E6QdZPSlvvrktE8\"\n",
    "    oauth_token = \"1036998187880460289-K5YKVDBIlK5e1bsgsxzvmGk2z2BzTO\"\n",
    "    oauth_token_secret = \"ogxeyO9mgweKrFEOZhn638EvTVvZjn06ynVwviShsVOzZ\"\n",
    "    \n",
    "    return app_key, app_secret, oauth_token, oauth_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresExtraction2(features_file):\n",
    "    convertedArray = json.load(open(features_file, 'r'))\n",
    "    final = []\n",
    "    textFeatures = []\n",
    "    \n",
    "    for i, j in enumerate(convertedArray):\n",
    "        tempDict = {}\n",
    "        tempDict['id'] = j['id']\n",
    "        tempDict['date'] = j['created_at']\n",
    "        tempDict['username'] = j['user']['screen_name']\n",
    "        tempDict['hashtags'] = j['entities']['hashtags']\n",
    "        tempDict['user_mentions'] = j['entities']['user_mentions']\n",
    "        tempDict['favorites'] = j['favorite_count']\n",
    "        \n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            textFeatures.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            textFeatures.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            textFeatures.append(j['text'])\n",
    "        \n",
    "        final.append(tempDict)\n",
    "    \n",
    "    return final, textFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorization(key):\n",
    "    if key == 'e' or 'E':\n",
    "        app_key = \"jGVy1ilV55A7P24eJHuousM1b\"\n",
    "        app_secret = \"wrPOxGirrXEUQsU7Pk0jqaqt7nLYq10TmR0E6QdZPSlvvrktE8\"\n",
    "        oauth_token = \"1036998187880460289-K5YKVDBIlK5e1bsgsxzvmGk2z2BzTO\"\n",
    "        oauth_token_secret = \"ogxeyO9mgweKrFEOZhn638EvTVvZjn06ynVwviShsVOzZ\"\n",
    "        return app_key, app_secret, oauth_token, oauth_token_secret\n",
    "    elif key == 'v' or 'V':\n",
    "        app_key = \"d\"\n",
    "    else:\n",
    "        print(\"Wrong identification key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cred_Processing(tweetsFile):\n",
    "    idRegEx = re.compile(r\".*ID=\")\n",
    "    endElRegEX = re.compile(r\"'.*\")\n",
    "    output = open(\"outputTweets.json\", \"w\")\n",
    "    \n",
    "    with open(tweetsFile, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        \n",
    "        for line in f:\n",
    "            topicData = line.split(\"\\t\")\n",
    "            \n",
    "            topicKey = topicData[0]\n",
    "            topicTerms = topicData[1]\n",
    "            topicTweetCount = topicData[2]\n",
    "            tweetIdList = topicData[3]\n",
    "            \n",
    "            print(topicKey)\n",
    "            \n",
    "            realTweetsIds = []\n",
    "            \n",
    "            idElements = tweetIdList.split(\"),\")\n",
    "            for element in idElements:\n",
    "                elArr = element.split(\",\")\n",
    "                idEl = list(filter(lambda x: \"ID\" in x, elArr))[0]\n",
    "                idEl = idRegEx.sub(\"\", idEl)\n",
    "                idEl = endElRegEX.sub(\"\", idEl)\n",
    "                \n",
    "                realTweetsIds.append(int(idEl))\n",
    "                \n",
    "            realTweetsIds = list(set(realTweetsIds))\n",
    "            \n",
    "            topicMap = {\n",
    "                \"key\" : topicKey,\n",
    "                \"terms\" : topicTerms.split(\",\"),\n",
    "                \"count\" : topicTweetCount, \n",
    "                \"tweets\" : realTweetsIds\n",
    "            }\n",
    "            \n",
    "            json.dump(topicMap, output, sort_keys=True)\n",
    "            output.write(\"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_processing(ratingFile, tweetsFile):\n",
    "    tweetsMap = {}\n",
    "    \n",
    "    with open(tweetsFile, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                tweetData = json.loads(line)\n",
    "                tweetsMap[tweetData[\"key\"]] = tweetData\n",
    "            except:\n",
    "                print(\"Error\")\n",
    "            \n",
    "    output = open(r\"C:\\Users\\vdoan\\Documents\\College\\SummerResearch\\SmResearch\\Data\\CREDBANK\\finalOutput.json\", \"w\")\n",
    "    \n",
    "    with open(ratingFile, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        \n",
    "        for line in f:\n",
    "            topicData = line.split(\"\\t\")\n",
    "            topicKey = topicData[0]\n",
    "            topicTerms = topicData[1]\n",
    "            ratings = topicData[2]\n",
    "            reasons = topicData[3]\n",
    "            \n",
    "            try:\n",
    "                ratings = list(map(lambda x: int(x.strip().replace(\"'\", \"\")), ratings.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")))\n",
    "                ratings = np.array(ratings)\n",
    "                tweetsMap[topicKey][\"mean\"] = np.mean(ratings)\n",
    "                print(\"Inserting ratings for {}\".format(topicKey))\n",
    "                tweetsMap[topicKey][\"ratings\"] = ratings.tolist()\n",
    "                print(\"Success\")\n",
    "                print(\"Inserting mean for {}\".format(topicKey))\n",
    "                tweetsMap[topicKey][\"mean\"] = np.mean(ratings)\n",
    "                print(\"Success\")\n",
    "                topicMap = tweetsMap[topicKey]\n",
    "                print(topicMap[\"key\"], topicMap[\"mean\"])\n",
    "                json.dump(topicMap, output, sort_keys=True)\n",
    "                output.write(\"\\n\")\n",
    "            except:\n",
    "                print(\"{} is missing.\".format(topicKey))\n",
    "            \n",
    "            \n",
    "            \n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryConversion(outputFile):\n",
    "    labels = {}\n",
    "    with open(outputFile, 'r') as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            if(tweet['mean'] >= 1.9):\n",
    "                labels[tweet['key']] = 1\n",
    "            elif(tweet['mean'] <= 1.467):\n",
    "                labels[tweet['key']] = 0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Retweet and date\n",
    "def singleMessageFeatureExtraction(tweet_json):\n",
    "    #print(tweet_json)\n",
    "    retweets = 0\n",
    "    try:\n",
    "        retweets = tweet_json[\"tweet\"][\"retweet_status\"]\n",
    "        retweets = 1\n",
    "    except:\n",
    "        retweets = 0\n",
    "    date = tweet_json[\"tweet\"][\"created_at\"].split(\" \")[0]\n",
    "    dates = []\n",
    "    if date == \"Mon\":\n",
    "        dates = [1,0,0,0,0,0,0]\n",
    "    elif date == \"Tue\":\n",
    "        dates = [0,1,0,0,0,0,0]\n",
    "    elif date == \"Wed\":\n",
    "        dates = [0,0,1,0,0,0,0]\n",
    "    elif date == \"Thu\":\n",
    "        dates = [0,0,0,1,0,0,0]\n",
    "    elif date == \"Fri\":\n",
    "        dates = [0,0,0,0,1,0,0]\n",
    "    elif date == \"Sat\":\n",
    "        dates = [0,0,0,0,0,1,0]\n",
    "    elif date == \"Sun\":\n",
    "        dates = [0,0,0,0,0,0,1]\n",
    "    return retweets, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unshorten_url(url):\n",
    "    return requests.head(url, allow_redirects=True).url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structuralFeatureExtraction(topics, threshold):\n",
    "    tweetsFeatures = None\n",
    "    np.array(tweetsFeatures)\n",
    "    count = 0\n",
    "    for key in topics:\n",
    "        print(\"Starting feature extraction for topic {} of {}\".format(count, len(topics)))\n",
    "        topicFeatures = []\n",
    "        \n",
    "        averageLength = 0\n",
    "        frequencyHashtags = 0\n",
    "        frequencyMedia = 0\n",
    "        frequencyMentions = 0\n",
    "        frequencyRetweets = 0\n",
    "        frequencyWebLinks = 0\n",
    "        \n",
    "        for tweet in topics[key]:\n",
    "            \n",
    "            print(\"Extracting information from tweet \" + str(tweet['id']))\n",
    "            #Length of characters\n",
    "            print(\"Adding Length of charactrs\")\n",
    "            averageLength = averageLength + len(tweet['tweet']['text'])\n",
    "            \n",
    "            #Frequency of Hashtags\n",
    "            print(\"Adding frequency of Hashtags\")\n",
    "            frequencyHashtags = frequencyHashtags + len(tweet['tweet']['entities']['hashtags'])\n",
    "                                \n",
    "            #Frequency of Media\n",
    "            print(\"Adding frequency of Media\")\n",
    "            #frequencyMedia = frequencyMedia + len(tweet['tweet']['entities']['media'])\n",
    "            \n",
    "            #Frequency of Mentions\n",
    "            print(\"Adding Frequency of Mentions\")\n",
    "            frequencyMentions = frequencyMentions + len(tweet['tweet']['entities']['user_mentions'])\n",
    "            \n",
    "            #Frequency of Retweets\n",
    "            print(\"Adding Frequency of Retweets\")\n",
    "            frequencyRetweets = frequencyRetweets\n",
    "            \n",
    "            #Frequency of Web Links\n",
    "            print(\"Adding Frequency of Web Links\")\n",
    "            frequencyWebLinks = frequencyWebLinks + len(tweet['tweet']['entities']['urls'])\n",
    "        \n",
    "        #Appending features to list\n",
    "        topicFeatures.append(len(topics[key]))\n",
    "        topicFeatures.append(averageLength)\n",
    "        topicFeatures.append(averageLength/len(topics[key]))\n",
    "        topicFeatures.append(frequencyHashtags)\n",
    "        topicFeatures.append(frequencyHashtags/len(topics[key]))\n",
    "        topicFeatures.append(frequencyMedia)\n",
    "        topicFeatures.append(frequencyMedia/len(topics[key]))\n",
    "        topicFeatures.append(frequencyMentions)\n",
    "        topicFeatures.append(frequencyMentions/len(topics[key]))\n",
    "        topicFeatures.append(frequencyRetweets)\n",
    "        topicFeatures.append(frequencyRetweets/len(topics[key]))\n",
    "        topicFeatures.append(frequencyWebLinks)\n",
    "        topicFeatures.append(frequencyWebLinks/len(topics[key]))\n",
    "        \n",
    "        #Appending feataures to features array\n",
    "        topicFeatures = [topicFeatures]\n",
    "        if(count == 0):\n",
    "            tweetsFeatures = topicFeatures\n",
    "            print(\"Length of array: {}\".format(len(tweetsFeatures[count])))\n",
    "        else:\n",
    "            tweetsFeatures = np.append(tweetsFeatures, topicFeatures, axis = 0)\n",
    "            print(tweetsFeatures.shape)\n",
    "        \n",
    "        print(\"Finished feature extraction for tweet {} of {}\".format(count, len(tweets)))\n",
    "        count += 1\n",
    "        \n",
    "    return tweetsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k  = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentFeatureExtraction(topics):\n",
    "    \n",
    "    tweetsFeatures = None\n",
    "    np.array(tweetsFeatures)\n",
    "    \n",
    "    first = True\n",
    "    first_pronouns = set([\"i\", \"we\"])\n",
    "    second_pronouns = set([\"you\"])\n",
    "    third_pronouns = set([\"he\", \"him,\" \"she\", \"her\", \"they\", \"them\", \"it\"])\n",
    "    \n",
    "    count2 = 0\n",
    "    tweetsFeatures = np.array([])\n",
    "    #prop for proportion, fr for frequency \n",
    "    total_no_tweets = 0\n",
    "        \n",
    "    fr_quest_mark = 0\n",
    "    fr_excl_mark = 0\n",
    "\n",
    "    fr_emot_smile = 0 \n",
    "        \n",
    "    fr_tweets_hashtag = 0\n",
    "        \n",
    "    fr_pron_first = 0\n",
    "    fr_pron_sec = 0\n",
    "    fr_pron_third = 0\n",
    "        \n",
    "    #Polarity = Average sentiment score of tweets \n",
    "    avg_polarity = 0 \n",
    "    avg_objectivity = 0\n",
    "    disagreement = 0\n",
    "    \n",
    " \n",
    "    for topic in topics: \n",
    "        current_topic = topics[topic]\n",
    "        total_no_tweets = len(current_topic)\n",
    "        for tweet in current_topic: \n",
    "            split_tweet = tweet[\"tweet\"][\"text\"].split()\n",
    "            split_tweet_word = set(tweet[\"tweet\"][\"text\"].split(\" \"))\n",
    "\n",
    "            if '?' in split_tweet:\n",
    "                fr_quest_mark += 1\n",
    "            if '!' in split_tweet:\n",
    "                fr_excl_mark += 1\n",
    "            if '😊' in split_tweet:\n",
    "                fr_emot_smile += 1\n",
    "            if '#' in split_tweet:\n",
    "                fr_tweets_hashtag += 1\n",
    "            \n",
    "            if (first_pronouns & split_tweet_word):\n",
    "                fr_pron_first += 1\n",
    "            if (second_pronouns & split_tweet_word):\n",
    "                fr_pron_sec += 1\n",
    "            if (third_pronouns & split_tweet_word):\n",
    "                fr_pron_third += 1 \n",
    "                \n",
    "            \n",
    "            tweetSentiment = TextBlob(tweet[\"tweet\"][\"text\"])\n",
    "            sentimentScore = tweetSentiment.sentiment[0]\n",
    "            objectivityScore = tweetSentiment.sentiment[1]\n",
    "            avg_polarity += sentimentScore\n",
    "            avg_objectivity += objectivityScore\n",
    "                \n",
    "                \n",
    "                \n",
    "        prop_quest_mark = fr_quest_mark / total_no_tweets\n",
    "        prop_excl_mark = fr_excl_mark / total_no_tweets\n",
    "        prop_emot_smile = fr_emot_smile / total_no_tweets\n",
    "        prop_tweets_hashtag = fr_tweets_hashtag / total_no_tweets\n",
    "\n",
    "        \n",
    "        prop_pron_first = fr_pron_first / total_no_tweets\n",
    "        prop_pron_sec = fr_pron_sec / total_no_tweets\n",
    "        prop_pron_third = fr_pron_third / total_no_tweets\n",
    "        \n",
    "        avg_polarity = avg_polarity / total_no_tweets\n",
    "        avg_objectivity = avg_objectivity / total_no_tweets        \n",
    "           \n",
    "        disagreement = 0\n",
    "        current_topic_feature = []\n",
    "    \n",
    "        current_topic_feature.append(avg_polarity)\n",
    "        current_topic_feature.append(avg_objectivity)\n",
    "        current_topic_feature.append(disagreement)\n",
    "        current_topic_feature.append(fr_quest_mark)\n",
    "        current_topic_feature.append(prop_quest_mark)\n",
    "        current_topic_feature.append(fr_excl_mark)\n",
    "        current_topic_feature.append(prop_excl_mark)\n",
    "        current_topic_feature.append(fr_emot_smile)\n",
    "        current_topic_feature.append(prop_emot_smile)\n",
    "        current_topic_feature.append(fr_pron_first)\n",
    "        current_topic_feature.append(prop_pron_first)\n",
    "        current_topic_feature.append(fr_pron_sec)\n",
    "        current_topic_feature.append(prop_pron_sec)\n",
    "        current_topic_feature.append(fr_pron_third)\n",
    "        current_topic_feature.append(prop_pron_third)\n",
    "    \n",
    "        topicFeatures = [current_topic_feature]\n",
    "        if(count2 == 0):\n",
    "            tweetsFeatures = topicFeatures\n",
    "            print(\"Length of array: {}\".format(len(tweetsFeatures[count2])))\n",
    "        else:\n",
    "            tweetsFeatures = np.append(tweetsFeatures, topicFeatures, axis = 0)\n",
    "            print(tweetsFeatures.shape)\n",
    "        count2 = count2 + 1\n",
    "    \n",
    "    return tweetsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userFeatureExtraction(topics):\n",
    "    tweetsFeatures = None\n",
    "    np.array(tweetsFeatures)\n",
    "    \n",
    "    first = True\n",
    "    userFeatures = None\n",
    "    count2 = 0\n",
    "    tweetsFeatures = np.array([])\n",
    "    #Avg date create\n",
    "    avg_date_user_create = 0\n",
    "        \n",
    "    #Average follower count\n",
    "    avg_follower_cnt = 0\n",
    "    \n",
    "    #Average friend count\n",
    "    avg_friend_cnt = 0 \n",
    "    \n",
    "    #Average authored status count \n",
    "    avg_aut_stat_cnt = 0\n",
    "    \n",
    "    #Frequency of verified author \n",
    "    fr_veri_aut = 0 \n",
    "    \n",
    "    #Author of the First Tweet is Verified\n",
    "    fr_first_veri = 0\n",
    "    \n",
    "        \n",
    "    avg_date_tweet_create = 0\n",
    "\n",
    "    for topic in topics:\n",
    "        no_tweet = len(topic)\n",
    "        for tweet in topics[topic]:\n",
    "            \n",
    "            #Average account age format : \"created_at\": \"Fri Jun 26 07:47:06 +0000 2009\"\n",
    "            user_created_string = tweet['tweet']['user']['created_at']\n",
    "            tweet_created_string = tweet['tweet']['created_at']\n",
    "            user_created_string_list = user_created_string.split(\" \")\n",
    "            tweet_created_string_list = tweet_created_string.split(\" \")\n",
    "            day_user = user_created_string_list[2]\n",
    "            month_user = 0\n",
    "            year_user = user_created_string_list[5]\n",
    "            \n",
    "            day_tweet = tweet_created_string_list[2]\n",
    "            month_tweet = 0\n",
    "            year_tweet = tweet_created_string_list[5]\n",
    "\n",
    "            if (user_created_string_list[1] == 'Jan'):\n",
    "                month_user = 1\n",
    "            elif (user_created_string_list[1] == 'Feb'):\n",
    "                month_user = 2\n",
    "            elif (user_created_string_list[1] == 'Mar'):\n",
    "                month_user = 3\n",
    "            elif (user_created_string_list[1] == 'Apr'):\n",
    "                month_user = 4\n",
    "            elif (user_created_string_list[1] == 'May'):\n",
    "                month_user = 5\n",
    "            elif (user_created_string_list[1] == 'Jun'):\n",
    "                month_user = 6\n",
    "            elif (user_created_string_list[1] == 'Jul'):\n",
    "                month_user = 7\n",
    "            elif (user_created_string_list[1] == 'Aug'):\n",
    "                month_user = 8\n",
    "            elif (user_created_string_list[1] == 'Sep'):\n",
    "                month_user = 9\n",
    "            elif (user_created_string_list[1] == 'Oct'):\n",
    "                month_user = 10\n",
    "            elif (user_created_string_list[1] == 'Nov'):\n",
    "                month_user = 11\n",
    "            elif (user_created_string_list[1] == 'Dec'):\n",
    "                month_user = 12\n",
    "                \n",
    "                \n",
    "            if (tweet_created_string_list[1] == 'Jan'):\n",
    "                month_tweet = 1\n",
    "            elif (tweet_created_string_list[1] == 'Feb'):\n",
    "                month_tweet = 2\n",
    "            elif (tweet_created_string_list[1] == 'Mar'):\n",
    "                month_tweet = 3\n",
    "            elif (tweet_created_string_list[1] == 'Apr'):\n",
    "                month_tweet = 4\n",
    "            elif (tweet_created_string_list[1] == 'May'):\n",
    "                month_tweet = 5\n",
    "            elif (tweet_created_string_list[1] == 'Jun'):\n",
    "                month_tweet = 6\n",
    "            elif (tweet_created_string_list[1] == 'Jul'):\n",
    "                month_tweet = 7\n",
    "            elif (tweet_created_string_list[1] == 'Aug'):\n",
    "                month_tweet = 8\n",
    "            elif (tweet_created_string_list[1] == 'Sep'):\n",
    "                month_tweet = 9\n",
    "            elif (tweet_created_string_list[1] == 'Oct'):\n",
    "                month_tweet = 10\n",
    "            elif (tweet_created_string_list[1] == 'Nov'):\n",
    "                month_tweet = 11\n",
    "            elif (tweet_created_string_list[1] == 'Dec'):\n",
    "                month_tweet = 12\n",
    "            \n",
    "            user_created_date = date(int(year_user),int(month_user),int(day_user))\n",
    "            tweet_created_date = date(int(year_tweet),int(month_tweet),int(day_tweet))\n",
    "            current_date = date.today()\n",
    "            user_delta = current_date - user_created_date\n",
    "            tweet_delta = current_date - tweet_created_date\n",
    "            avg_date_user_create += user_delta.days\n",
    "            avg_date_tweet_create += tweet_delta.days\n",
    "            \n",
    "            \n",
    "            \n",
    "            follower_count = tweet['tweet']['user']['followers_count']\n",
    "        \n",
    "            friend_count = tweet['tweet']['user']['friends_count']\n",
    "        \n",
    "            aut_stat_count = tweet['tweet']['user']['statuses_count']\n",
    "        \n",
    "            #0 for false, 1 for true\n",
    "            is_verified = 0\n",
    "        \n",
    "            if (tweet['tweet']['user']['verified'] == True):\n",
    "                is_verified = 1\n",
    "            else:\n",
    "                is_verified = 0\n",
    "            \n",
    "            if (tweet['tweet']['in_reply_to_status_id'] != None) and (tweet['tweet']['user']['verified'] == True):\n",
    "                fr_first_veri += 1\n",
    "            \n",
    "                                                                      \n",
    "            avg_follower_cnt += follower_count\n",
    "            avg_friend_cnt += friend_count\n",
    "            avg_aut_stat_cnt += aut_stat_count\n",
    "            fr_veri_aut += is_verified\n",
    "                                     \n",
    "        G = nx.Graph()\n",
    "        current_users = []\n",
    "                                                                    \n",
    "        for tweet in topics[topic]:\n",
    "            if tweet['tweet']['user']['id'] not in current_users:\n",
    "                current_users.append(tweet['tweet']['user']['id'])\n",
    "                G.add_node(tweet['tweet']['user']['id'])                  \n",
    "                                                                    \n",
    "        for tweet in topics[topic]:\n",
    "            for i in tweet['tweet']['entities']['user_mentions']:\n",
    "                mention_id = i['id']\n",
    "                G.add_edge(tweet['tweet']['user']['id'], mention_id)\n",
    "    \n",
    "            if 'retweeted_status' in i:\n",
    "                retweet_id = i['tweet']['retweeted_status']['id']\n",
    "                G.add_edge(tweet['tweet']['user']['id'], retweet_id)                                        \n",
    "                                                                    \n",
    "                                                                      \n",
    "        avg_date_user_create = avg_date_user_create / no_tweet                                                              \n",
    "        avg_follower_cnt = avg_follower_cnt / no_tweet\n",
    "        avg_friend_cnt = avg_friend_cnt / no_tweet\n",
    "        avg_aut_stat_cnt = avg_aut_stat_cnt / no_tweet\n",
    "        avg_date_tweet_create = avg_date_tweet_create / no_tweet                                                    \n",
    "        \n",
    "        current_topic_feature = []\n",
    "        \n",
    "        current_topic_feature.append(avg_date_user_create)                                                              \n",
    "        current_topic_feature.append(avg_follower_cnt)\n",
    "        current_topic_feature.append(avg_friend_cnt)\n",
    "        current_topic_feature.append(avg_aut_stat_cnt)\n",
    "        current_topic_feature.append(fr_veri_aut)\n",
    "        current_topic_feature.append(fr_first_veri)\n",
    "        current_topic_feature.append(density(G)) \n",
    "\n",
    "        print(\"Current topic extracted features: \")\n",
    "        print(avg_date_user_create)\n",
    "        print(avg_follower_cnt)\n",
    "        print(avg_friend_cnt)\n",
    "        print(avg_aut_stat_cnt)\n",
    "        print(fr_veri_aut)\n",
    "        print(fr_first_veri)\n",
    "        print(density(G))\n",
    "        print()\n",
    "        \n",
    "        topicFeatures = [current_topic_feature]\n",
    "        if(count2 == 0):\n",
    "            tweetsFeatures = topicFeatures\n",
    "            print(\"Length of array: {}\".format(len(tweetsFeatures[count2])))\n",
    "        else:\n",
    "            tweetsFeatures = np.append(tweetsFeatures, topicFeatures, axis = 0)\n",
    "            print(tweetsFeatures.shape)\n",
    "        count2 = count2 + 1\n",
    "    \n",
    "    return tweetsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporalFeatureExtraction(topics):\n",
    "    \n",
    "    counts = 1\n",
    "    count = 1\n",
    "    first = True\n",
    "    firstArray = True\n",
    "    temporalFeatures = []\n",
    "    tweetsFeatures = np.array([])\n",
    "    count2 = 0\n",
    "    #first need to sort the tweets\n",
    "    for k in topics:\n",
    "        print(\"Starting feature extraction for topic {} of {}\".format(counts, len(topics)))\n",
    "    \n",
    "        #Sort tweets\n",
    "        sortedArray = sorted(topics[k], key=lambda x: datetime.strptime(x['tweet']['created_at'],\"%a %b %d %H:%M:%S +0000 %Y\"))\n",
    "        time = 0\n",
    "        \n",
    "        followers = 0\n",
    "        friends = 0\n",
    "        status = 0\n",
    "        perMin = 0\n",
    "        \n",
    "        for tweet in sortedArray:\n",
    "            if(first == True):\n",
    "                time = timeConvert(tweet['tweet']['created_at'])\n",
    "#             print(\"Beginning Time: \" + str(time))\n",
    "#             print(\"Current Time: \" + str(timeConvert(tweet['tweet']['created_at'])) + \", In seconds: \" + str(timeConvert(tweet['tweet']['created_at']) - time))\n",
    "#             print(\"Time difference: \" + str(timeConvert(tweet['tweet']['created_at']) - time))\n",
    "            if((timeConvert(tweet['tweet']['created_at']) - time) < 60 and (timeConvert(tweet['tweet']['created_at']) >= time)):\n",
    "                print(\"In section: \" + str(count))\n",
    "                print(\"Time difference: \" + str(timeConvert(tweet['tweet']['created_at']) - time))\n",
    "                print(tweet['tweet']['created_at'])\n",
    "                first = False\n",
    "            elif(timeConvert(tweet['tweet']['created_at']) > time):\n",
    "                if(((time + timeConvert(tweet['tweet']['created_at'])) - time) < 60) :\n",
    "                    print(\"In section: \" + str(count))\n",
    "                    print(\"Time difference: \" + str(timeConvert(tweet['tweet']['created_at']) - time))\n",
    "                    print(tweet['tweet']['created_at'])\n",
    "                    first = False\n",
    "                else:\n",
    "                    count = count + 1\n",
    "                    print(\"Start of Section: \" + str(count))\n",
    "                    print()\n",
    "                    print(\"Time difference for change is due to: \" + str(timeConvert(tweet['tweet']['created_at']) - time))\n",
    "                    time = timeConvert(tweet['tweet']['created_at'])\n",
    "\n",
    "                    print(tweet['tweet']['created_at'])\n",
    "                    print(\"Time difference: \" + str(timeConvert(tweet['tweet']['created_at']) - time))\n",
    "            else:\n",
    "                count = count + 1\n",
    "                print(\"Start of Section: \" + str(count))\n",
    "                print()\n",
    "                print(\"Time difference for change is due to: \" + str(timeConvert(tweet['tweet']['created_at']) - time))\n",
    "                time = timeConvert(tweet['tweet']['created_at'])\n",
    "\n",
    "                print(tweet['tweet']['created_at'])\n",
    "                print(\"Time difference: \" + str(timeConvert(tweet['tweet']['created_at']) - time))\n",
    "            \n",
    "            #Account Age\n",
    "            \n",
    "            #Difference Between account age and tweet publication time\n",
    "            \n",
    "            #Author's Follower\n",
    "            followers = followers + tweet['tweet']['user'][\"followers_count\"]\n",
    "            print(\"Current Followers: \" + str(followers))\n",
    "            #Author's Friends\n",
    "            friends = friends + tweet['tweet']['user'][\"friends_count\"]\n",
    "            print(\"Current Friends: \" + str(friends))\n",
    "            #Author's Statuses\n",
    "            if(tweet['tweet']['user'][\"verified\"] == \"True\"):\n",
    "                status = status + 1\n",
    "            print(\"Current Amount of Statusese: \" + str(status))\n",
    "            #Number of tweets per minute\n",
    "            perMin = perMin + 1\n",
    "            print(\"Current Amount of Tweets: \" + str(perMin))\n",
    "        \n",
    "        counts = counts + 1\n",
    "        current_topic_features = []\n",
    "        current_topic_features.append(followers)\n",
    "        current_topic_features.append(friends)\n",
    "        current_topic_features.append(status)\n",
    "        current_topic_features.append(perMin)\n",
    "        print(current_topic_features)\n",
    "        \n",
    "        topicFeatures = [current_topic_features]\n",
    "        if(count2 == 0):\n",
    "            tweetsFeatures = topicFeatures\n",
    "            print(\"Length of array: {}\".format(len(tweetsFeatures[count2])))\n",
    "        else:\n",
    "            tweetsFeatures = np.append(tweetsFeatures, topicFeatures, axis = 0)\n",
    "            print(tweetsFeatures.shape)\n",
    "        count2 = count2 + 1\n",
    "#         if(firstArray == True):\n",
    "#             temporalFeatures = np.array([current_topic_features])\n",
    "#             firstArray = False\n",
    "#             print(\"First time\")\n",
    "#         else:\n",
    "#             print(\"Other times\")\n",
    "#             features = np.array([current_topic_features])\n",
    "#             temporalFeatures = np.concatenate((temporalFeatures, features), axis=1)\n",
    "        \n",
    "        #print(\"Shape of features: \" + str(temporalFeatures.size))\n",
    "            \n",
    "    \n",
    "    return tweetsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveFeatures(topics, threshold):\n",
    "    structuralArray = structuralFeatureExtraction(topics, threshold)\n",
    "    contentArray = contentFeatureExtraction(topics)\n",
    "    userArray = userFeatureExtraction(topics)\n",
    "    temporalArray = temporalFeatureExtraction(topics)\n",
    "    \n",
    "    print(\"contentArray: \", len(contentArray[0]))\n",
    "    print(\"structuralArray: \", len(structuralArray[0]))\n",
    "    print(\"userArray: \", len(userArray[0]))\n",
    "    print(\"temporalArray: \", len(temporalArray[0]))\n",
    "    featureArray = np.hstack((contentArray, structuralArray))\n",
    "    print(\"featureArray: \", featureArray.shape)\n",
    "    featureArray_2 = np.hstack((featureArray, userArray))\n",
    "    print(\"featureArray: \", featureArray_2.shape)\n",
    "    featureArray_3 = np.hstack((featureArray_2, temporalArray))\n",
    "    print(\"featureArray: \", featureArray_3.shape)\n",
    "    print(\"Feature Created.\")\n",
    "    return featureArray_3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeConvert(time):\n",
    "    time = datetime.strptime(time, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    return timedelta(minutes = time.minute).seconds + time.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             print(\"Adding words\")\n",
    "#             #Length of words 2\n",
    "#             indiTweetFeature.append(len(tweet.split(\" \")))\n",
    "        \n",
    "#             print(\"Adding question mark\")\n",
    "#             #Contains question mark 3\n",
    "#             results = collections.Counter(tweet)\n",
    "#             if(results['?'] > 1):\n",
    "#                 indiTweetFeature.append(1)\n",
    "#             else:\n",
    "#                 indiTweetFeature.append(0)\n",
    "\n",
    "#             print('Adding exclamation mark')\n",
    "#             #Contains exclamation 4\n",
    "#             if(results['!'] > 1):\n",
    "#                 indiTweetFeature.append(1)\n",
    "#             else:\n",
    "#                 indiTweetFeature.append(0)\n",
    "\n",
    "#             print(\"Adding smiles\")\n",
    "#             #Contains emoticon smile 5\n",
    "#             emojis = \"😊\"\n",
    "#             indiTweetFeature.append(tweet.count(emojis))\n",
    "\n",
    "#             print(\"!?\")\n",
    "#             #Contains multiple question or exclamation marks 6\n",
    "#             if results['?'] and results['!'] > 1:\n",
    "#                 indiTweetFeature.append(1)\n",
    "#             else:\n",
    "#                 indiTweetFeature.append(0)\n",
    "\n",
    "#             print(\"Adding first person pronouns\")\n",
    "#             #Contains pronoun first, sceond, third 7\n",
    "#             first_pronouns = [\"i\", \"we\"]\n",
    "#             tweetSplit = tweet.split(\" \")\n",
    "#             for x in range(len(tweetSplit)):\n",
    "#                 if(tweetSplit[x].lower() in first_pronouns):\n",
    "#                     indiTweetFeature.append(1)\n",
    "#                     break;\n",
    "#                 if(x == len(tweetSplit)-1):\n",
    "#                     indiTweetFeature.append(0)\n",
    "\n",
    "#             print(\"Adding second person pronouns\")\n",
    "#             second_pronouns = [\"you\"]\n",
    "#             tweetSplit = tweet.split(\" \")\n",
    "#             for x in range(len(tweetSplit)):\n",
    "#                 if(tweetSplit[x].lower() in second_pronouns):\n",
    "#                     indiTweetFeature.append(1)\n",
    "#                     break;\n",
    "#                 if(x == len(tweetSplit)-1):\n",
    "#                     indiTweetFeature.append(0)\n",
    "\n",
    "#             print(\"Adding third person pronouns\")\n",
    "#             third_pronouns = [\"he\", \"him,\" \"she\", \"her\", \"they\", \"them\", \"it\"]\n",
    "\n",
    "#             tweetSplit = tweet.split(\" \")\n",
    "#             for x in range(len(tweetSplit)):\n",
    "#                 if(tweetSplit[x].lower() in third_pronouns):\n",
    "#                     indiTweetFeature.append(1)\n",
    "#                     break;\n",
    "#                 if(x == len(tweetSplit)-1):\n",
    "#                     indiTweetFeature.append(0)\n",
    "\n",
    "#             print(\"Adding uppercase\")\n",
    "#             #Count uppercase letters 8\n",
    "#             upperLetterCount = 0\n",
    "#             for letter in tweet:\n",
    "#                 if(letter.isupper()):\n",
    "#                     upperLetterCount += 1\n",
    "#             indiTweetFeature.append(upperLetterCount)\n",
    "\n",
    "#             print(\"Adding url\")\n",
    "#             #Number of URLs 9\n",
    "#             url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+] |[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet) \n",
    "#             indiTweetFeature.append(len(url))\n",
    "        \n",
    "#             #Contains popular domain top 100,1000,10000\n",
    "#             print(\"Adding popular domain\")\n",
    "#             file = pd.read_csv('/media/eric/TOSHIBA EXT/University/Research/Data/top-1m.csv')\n",
    "#             try:\n",
    "#                 urls = tweet_json[count]['tweet']['entities']['urls']\n",
    "#             except:\n",
    "#                 print(tweet_json[count]['tweet'])\n",
    "#             top100 = 0\n",
    "#             top1000 = 0\n",
    "#             top10000 = 0\n",
    "\n",
    "#             for url in urls:\n",
    "#                 urlParsed = None\n",
    "#                 rank = None\n",
    "#                 try:\n",
    "#                     split_url = urlsplit(url['expanded_url'])   \n",
    "#                     shortUrl = ''\n",
    "#                     if(split_url.netloc[:4] == 'www.'):\n",
    "#                         shortUrl = split_url.netloc[4:]\n",
    "#                     else:\n",
    "#                         shortUrl = split_url.netloc\n",
    "#                     print(shortUrl)\n",
    "#                     print(file.loc[file['site'] == shortUrl]['rank'])\n",
    "#                     rank = int(file.loc[file['site'] == split_url.netloc]['rank'])\n",
    "#                 except Exception as e:\n",
    "#                     rank = 10001\n",
    "#                 if rank <= 100:\n",
    "#                     top100 = 1\n",
    "#                 if rank <= 1000:\n",
    "#                     top1000 = 1\n",
    "#                 if rank <= 10000:\n",
    "#                     top10000 = 1\n",
    "        \n",
    "#             indiTweetFeature.append(top100)\n",
    "#             indiTweetFeature.append(top1000)\n",
    "#             indiTweetFeature.append(top10000)\n",
    "                \n",
    "#             print(\"Adding mention\")\n",
    "#             #Contains user mention 10\n",
    "#             result = re.findall(\"(^|[^@\\w])@(\\w{1,15})\", tweet)\n",
    "#             if(len(result) >= threshold):\n",
    "#                 indiTweetFeature.append(1)\n",
    "#             else:\n",
    "#                 indiTweetFeature.append(0)\n",
    "    \n",
    "#             # Length of hashtags 11\n",
    "#             print(\"Adding hashtags\")\n",
    "#             hashtags = [i  for i in tweet.split() if i.startswith(\"#\") ]\n",
    "#             indiTweetFeature.append(len(hashtags))\n",
    "     \n",
    "#             #Contains stock symbol 12\n",
    "#             print(\"Addings stocks\")\n",
    "#             file = open('/media/eric/TOSHIBA EXT/University/Research/Data/nasdaqlisted.txt', 'r')\n",
    "#             filecontents = file.readlines()\n",
    "#             stock_symbols = []\n",
    "#             for line in filecontents:\n",
    "#                 splitline = line.split(\"|\")\n",
    "#                 stock_symbols.append(splitline[0])\n",
    "            \n",
    "#             del stock_symbols[0]\n",
    "        \n",
    "#             stockCount = 0\n",
    "#             for word in tweetSplit:\n",
    "#                 if(word in stock_symbols):\n",
    "#                     stockCount += 1\n",
    "        \n",
    "#             if(stockCount > 0):\n",
    "#                 indiTweetFeature.append(1)\n",
    "#             else:\n",
    "#                 indiTweetFeature.append(0)\n",
    "                \n",
    "#             retweets, date = singleMessageFeatureExtraction(tweet_json[count])\n",
    "#             print(\"Adding retweets\")\n",
    "#             indiTweetFeature.append(retweets)\n",
    "            \n",
    "#             print(\"Adding date\")\n",
    "#             indiTweetFeature = indiTweetFeature + date\n",
    "        \n",
    "#             print(\"Adding sentiment\")\n",
    "#             #Sentiment 13, 14, 15\n",
    "#             positiveWords = 0\n",
    "#             negativeWords= 0\n",
    "    \n",
    "    \n",
    "#             tweetSentiment = TextBlob(tweet)\n",
    "#             sentimentScore = tweetSentiment.sentiment[0]\n",
    "    \n",
    "#             for word in tweetSentiment:\n",
    "#                 word = TextBlob(word)\n",
    "#                 if(word.sentiment.polarity) > 0:\n",
    "#                     positiveWords = positiveWords + 1\n",
    "#                 elif(word.sentiment.polarity) < 0:\n",
    "#                     negativeWords = negativeWords + 1\n",
    "#             indiTweetFeature.append(positiveWords)\n",
    "#             indiTweetFeature.append(negativeWords)\n",
    "#             indiTweetFeature.append(sentimentScore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
